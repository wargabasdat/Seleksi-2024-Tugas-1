{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1>Data Scraping</h1>\n",
    "  <h2>Data Hunian Rumah dan Apartemen di Jakarta Pusat</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install dan import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # jika packages berikut belum terinstall, un-comment dan jalankan cell ini untuk menginstallnya\n",
    "# %pip install --upgrade pip --quiet\n",
    "# %pip install selenium beautifulsoup4 requests pandas ipython datetime --quiet\n",
    "\n",
    "# note: jika ada yang belum terinstall lakukan command di cell baru:\n",
    "# %pip install <nama package>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4                                import BeautifulSoup\n",
    "from selenium                           import webdriver\n",
    "from selenium.webdriver.chrome.options  import Options\n",
    "from selenium.webdriver.support.ui      import WebDriverWait\n",
    "from selenium.webdriver.support         import expected_conditions as EC\n",
    "from selenium.webdriver.common.by       import By\n",
    "from IPython.display                    import clear_output\n",
    "from datetime                           import datetime\n",
    "import requests, math, json, re, time, os, random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Header untuk scraping dan pattern untuk diambil di halaman web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__USER_AGENT__ = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "__BASE_URL__ = 'https://www.rumah123.com/'\n",
    "__EMAIL__ = '13522116@mahasiswa.itb.ac.id'\n",
    "__TOTAL_ITEM__ = 200\n",
    "__TIME_SLEEP__ = 5\n",
    "\n",
    "__HEADERS__ = {\n",
    "    'User-Agent': __USER_AGENT__,\n",
    "    'From': __EMAIL__\n",
    "}\n",
    "\n",
    "\"\"\" PATTERN \"\"\"\n",
    "__PATTERN_PROPERTY__    = r'^/properti/jakarta-pusat/.*'\n",
    "__PATTERN_AGENT__       = r'^/agen-properti/.*'\n",
    "__PATTERN_PAGINATION__  = r'.*page=\\d+'\n",
    "\n",
    "\"\"\" PATH STORING \"\"\"\n",
    "__CURRENT_DIRECTORY__   = os.getenv('CURRENT_DIR', os.getcwd())\n",
    "__PATH_FOLDER__         = os.path.abspath(os.path.join(__CURRENT_DIRECTORY__, '..', 'data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Request ke halaman web dan parsing menggunakan regex untuk mendapatkan konten yang diinginkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request ke URL untuk mengambil content\n",
    "def get_content(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# mengambil link yang sesuai dengan pola\n",
    "def get_parse_links(page_content, pattern):\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    urls = soup.find_all(\"a\", href=re.compile(pattern))\n",
    "    filtered_urls = [f\"https://www.rumah123.com{url['href']}\" for url in urls]\n",
    "    unique_links = list(set(filtered_urls))\n",
    "    return unique_links\n",
    "\n",
    "# mendapatkan jumlah page\n",
    "def get_max_page(page_content):\n",
    "    page_link = get_parse_links(page_content, __PATTERN_PAGINATION__)\n",
    "    page_numbers = [int(match.group(1)) for link in page_link if (match := re.search(r'page=(\\d+)', link))]\n",
    "    return max(page_numbers, default=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mengambil informasi untuk hunian (rumah dan apartemen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getter\n",
    "def get_id_iklan(url):\n",
    "    id_iklan = url.rstrip('/').split('/')[-1]\n",
    "    return id_iklan\n",
    "\n",
    "def get_prop_value(soup, label):\n",
    "    detail_element = soup.find(lambda tag: tag.name == 'p' and tag.get_text(strip=True) == label)\n",
    "    if detail_element:\n",
    "        sibling = detail_element.find_next_sibling('p')\n",
    "        return sibling.get_text(strip=True) if sibling else None\n",
    "    return None\n",
    "\n",
    "# mengambil atribut-atribut dari hunian\n",
    "def get_property(url, tipe_iklan, content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    tipe_properti           = get_prop_value(soup, 'Tipe Properti')\n",
    "    luas_bangunan           = get_prop_value(soup, 'Luas Bangunan')\n",
    "    kamar_tidur             = get_prop_value(soup, 'Kamar Tidur')\n",
    "    kamar_mandi             = get_prop_value(soup, 'Kamar Mandi')\n",
    "    sertifikat              = get_prop_value(soup, 'Sertifikat')\n",
    "    luas_tanah              = get_prop_value(soup, 'Luas Tanah')            if tipe_properti == 'Rumah' else -1\n",
    "    carport                 = get_prop_value(soup, 'Carport')               if tipe_properti == 'Rumah' else -1\n",
    "    kondisi_properti        = get_prop_value(soup, 'Kondisi Properti')      if tipe_properti == 'Apartemen' else -1\n",
    "    kondisi_perabotan       = get_prop_value(soup, 'Kondisi Perabotan')     if tipe_properti == 'Apartemen' else -1\n",
    "    periode_kepemilikan     = get_prop_value(soup, 'Periode Sewa')          if tipe_iklan == 'sewa' else -1\n",
    "\n",
    "    lokasi = soup.find('p', class_='text-sm text-gray-400')\n",
    "    if lokasi:\n",
    "        lokasi = lokasi.get_text(strip=True)\n",
    "        lokasi = lokasi.split(',')[0].strip() if ',' in lokasi else lokasi.strip()\n",
    "    else: return None\n",
    "\n",
    "    harga = soup.find('p', class_='text-sm text-primary mb-1 font-semibold')\n",
    "    if harga:\n",
    "        harga = harga.get_text(strip=True)\n",
    "        harga = re.search(r'Rp\\s(.*)', harga).group(1).strip() if harga else None\n",
    "    else: return None\n",
    "\n",
    "    diperbarui = soup.find('p', class_='text-3xs text-gray-400 mb-4')\n",
    "    if diperbarui:\n",
    "        diperbarui = diperbarui.get_text(strip=True)\n",
    "        diperbarui = re.search(r'Diperbarui(\\d{1,2} \\w+ \\d{4})', diperbarui).group(1)\n",
    "    else: return None\n",
    "\n",
    "    elements = soup.find_all('p', class_='w-1/2 flex items-center gap-4')\n",
    "    texts = [elem.find('span').get_text(strip=True) for elem in elements]\n",
    "    taman = 'Ya' if 'Taman' in texts else 'Tidak'\n",
    "\n",
    "    id_iklan = get_id_iklan(url)\n",
    "    \n",
    "    return {\n",
    "        'id_iklan'              : id_iklan,\n",
    "        'tipe_properti'         : tipe_properti,\n",
    "        'luas_bangunan'         : luas_bangunan,\n",
    "        'kamar_tidur'           : kamar_tidur,\n",
    "        'kamar_mandi'           : kamar_mandi,\n",
    "        'lokasi'                : lokasi,\n",
    "        'sertifikat'            : sertifikat,\n",
    "        'tipe_iklan'            : tipe_iklan,\n",
    "        'periode_kepemilikan'   : periode_kepemilikan,\n",
    "        'harga'                 : harga,\n",
    "        'diperbarui'            : diperbarui,\n",
    "        'luas_tanah'            : luas_tanah,\n",
    "        'carport'               : carport,\n",
    "        'taman'                 : taman,\n",
    "        'kondisi_properti'      : kondisi_properti,\n",
    "        'kondisi_perabotan'     : kondisi_perabotan\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mengambil informasi untuk agen dan perusahaannya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi driver\n",
    "def init_driver(user_agent, url):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument('--user-agent=' + user_agent)\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--disable-browser-side-navigation')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "# mengambil atribut-atribut dari agen dan perusahaan\n",
    "def get_agen(url, user_agent):\n",
    "    driver = init_driver(user_agent, url)\n",
    "    \n",
    "    try:\n",
    "        xpath       = \"//script[@type='application/json']\"\n",
    "        wait        = WebDriverWait(driver, 5)\n",
    "        occured     = EC.presence_of_element_located((By.XPATH, xpath))\n",
    "        script_tag  = wait.until(occured)\n",
    "\n",
    "        json_data   = script_tag.get_attribute('innerHTML')\n",
    "        data        = json.loads(json_data)\n",
    "        user_info   = data['props']['pageProps']['page']['user']\n",
    "        \n",
    "        return {\n",
    "            'id_agen'           : user_info['id'],\n",
    "            'nama_agen'         : user_info['info']['fullName'],\n",
    "            'nomor_telepon'     : user_info['info']['phone'],\n",
    "            'terjual'           : user_info['performance']['sold'],\n",
    "            'tersewa'           : user_info['performance']['rented'],\n",
    "            'nama_perusahaan'   : user_info['company']['name'],\n",
    "            'alamat'            : user_info['about']['addresses'][0]\n",
    "        }\n",
    "    \n",
    "    except Exception:\n",
    "        print(f'Error occurred while scraping\\nURL: {url}\\nCannot find element\\nContinuing scraping...\\n')\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display untuk status dan hasil scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__RED__     = '\\033[91m'\n",
    "__GREEN__   = '\\033[92m'\n",
    "__RESET__   = '\\033[0m'\n",
    "\n",
    "def display_log(count, total):\n",
    "    log_message = (\n",
    "        f'==> scraping in process: '\n",
    "        f'({__RED__}{count}{__RESET__}/{__GREEN__}{total}{__RESET__})\\n'\n",
    "    )\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(log_message)\n",
    "\n",
    "def display_result(count, total, timestamp):\n",
    "    log_message = (\n",
    "        f'{__RED__}............................{__RESET__}\\n'\n",
    "        f'............................\\n'\n",
    "        f'{__GREEN__}............................{__RESET__}\\n'\n",
    "        '==== scraping completed ====\\n\\n'\n",
    "        f'-> data obtained\\t\\t: ({__GREEN__}{count}{__RESET__}/{__GREEN__}{total}{__RESET__})\\n'\n",
    "        f'-> timestamp (started at)\\t: {timestamp}\\n'\n",
    "    )\n",
    "    print(log_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proses scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(url, headers, total_item, time_sleep):\n",
    "    timestamp       = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    count_jual      = math.ceil(total_item / 2)\n",
    "    count_sewa      = total_item - count_jual\n",
    "    tipe_iklan      = ['jual', 'sewa']\n",
    "    visited_urls    = set() \n",
    "    all_data        = []\n",
    "\n",
    "    display_log(all_data.__len__(), total_item)\n",
    "    for tipe in tipe_iklan:\n",
    "        count_limit     = count_jual if tipe == 'jual' else count_sewa\n",
    "        count_data      = 0\n",
    "        start_url       = f\"{url}{tipe}/jakarta-pusat/residensial/\"\n",
    "        page            = get_content(start_url, headers)\n",
    "\n",
    "        if page:\n",
    "            max_page        = get_max_page(page)\n",
    "            count_page      = random.randint(1, max_page - 21)\n",
    "\n",
    "            while count_data < count_limit:\n",
    "                if count_page <= max_page:\n",
    "                    complete_url = f\"{start_url}?page={str(count_page)}\"\n",
    "                    page = get_content(complete_url, headers)\n",
    "\n",
    "                    if page:\n",
    "                        items = get_parse_links(page, __PATTERN_PROPERTY__)\n",
    "\n",
    "                        for item in items:\n",
    "                            if item not in visited_urls:\n",
    "                                visited_urls.add(item)\n",
    "                                content = get_content(item, headers)\n",
    "\n",
    "                                if content:\n",
    "                                    data_property   = get_property(item, tipe, content)\n",
    "                                    if data_property:\n",
    "                                        url_agen        = get_parse_links(content, __PATTERN_AGENT__)\n",
    "                                        filtered_urls   = [url for url in url_agen if url.count('/') == 6][0]\n",
    "                                        data_agen       = get_agen(filtered_urls, __USER_AGENT__)\n",
    "\n",
    "                                        if data_agen:\n",
    "                                            data = {**data_property, **data_agen}\n",
    "                                            data['timestamp'] = timestamp\n",
    "                                            all_data.append(data)\n",
    "                                            count_data += 1\n",
    "                                            display_log(all_data.__len__(), total_item)\n",
    "\n",
    "                                            if count_data >= count_limit:\n",
    "                                                break\n",
    "\n",
    "                                else:\n",
    "                                    time.sleep(time_sleep)\n",
    "\n",
    "                    else:\n",
    "                        time.sleep(time_sleep)\n",
    "\n",
    "                    count_page += 1\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "    display_result(all_data.__len__(), __TOTAL_ITEM__, timestamp)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Menggabungkan data yang baru diperoleh dari scraping dengan data sebelumnya dan mengambil yang terbaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_existing(data):\n",
    "    PATH_FILE = os.path.join(__PATH_FOLDER__, 'data_raw.json')\n",
    "    if os.path.exists(PATH_FILE):\n",
    "        old_df = pd.read_json(PATH_FILE)\n",
    "    else:\n",
    "        old_df = pd.DataFrame()\n",
    "        \n",
    "    new_df      = pd.DataFrame(data)\n",
    "    latest_df   = pd.concat([new_df, old_df], ignore_index=True)\n",
    "    latest_df   = latest_df.drop_duplicates(subset='id_iklan', keep='first')\n",
    "    latest_df.to_json(PATH_FILE, orient='records', lines=False, indent=4, date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- main scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_scraping():\n",
    "    data = scraping(url        =   __BASE_URL__, \n",
    "                    headers    =   __HEADERS__,\n",
    "                    total_item =   __TOTAL_ITEM__,\n",
    "                    time_sleep =   __TIME_SLEEP__,\n",
    "         )\n",
    "    merge_existing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scraping in process: (\u001b[91m200\u001b[0m/\u001b[92m200\u001b[0m)\n",
      "\n",
      "\u001b[91m............................\u001b[0m\n",
      "............................\n",
      "\u001b[92m............................\u001b[0m\n",
      "==== scraping completed ====\n",
      "\n",
      "-> data obtained\t\t: (\u001b[92m200\u001b[0m/\u001b[92m200\u001b[0m)\n",
      "-> timestamp (started at)\t: 2024-08-02 07:31:11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# __MAIN__\n",
    "main_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
